{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras是基于Theano和TensorFlow的深度学习库，其特点如下：\n",
    "\n",
    "1、简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性）  \n",
    "2、支持CNN和RNN，或二者的结合  \n",
    "3、支持任意的链接方案（包括多输入和多输出训练）  \n",
    "4、无缝CPU和GPU切换\n",
    "\n",
    "Keras的核心数据结构是“模型”，模型是一种组织网络层的方式，Keras中有两种模型：  \n",
    "Sequential模型：序贯模型，也就是单输入单输出，一条路通到底，层与层之间只有相邻关系，跨层连接统统没有。    \n",
    "functional模型：函数模型\n",
    "\n",
    "第一步、选择模型 \n",
    "from keras.models import    \n",
    "Sequential model = Sequential()  \n",
    "\n",
    "第二步、搭建模型：将一些网络层通过.add()堆叠起来，就构成了一个模型：  \n",
    "from keras.layers import Dense, Activation  \n",
    "model.add(Dense(output_dim=64, input_dim=100))  \n",
    "model.add(Activation(\"relu\"))  \n",
    "model.add(Dense(output_dim=10))  \n",
    "model.add(Activation(\"softmax\"))  \n",
    "\n",
    "第三步、编译模型：compile()，需指明损失函数、优化器、评估指标，可自己定制优化器  \n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])  \n",
    "\n",
    "第四步、训练模型： fit（）操作 或 train_on_batch（）操作  \n",
    "fit（）操作：在训练数据上按batch进行一定次数的迭代训练，以拟合网络。  \n",
    "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)  \n",
    "train_on_batch（）操作：手动将一个个batch的数据送入网络中训练  \n",
    "model.train_on_batch(X_batch, Y_batch)  \n",
    "\n",
    "第五步、评估模型： evaluate返回损失值和compilie定义的评估指标、predict、predict_classes预测输出，predict_proba返回预测值的概率。  \n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)  \n",
    "classes = model.predict_classes(X_test, batch_size=32)  \n",
    "classes = model.predict(X_test)  \n",
    "proba = model.predict_proba(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型的保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型的结构、权重、配置\n",
    "from keras.models import load_model\n",
    " \n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    " \n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只保存模型的结构\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# save as JSON\n",
    "json_string = model.to_json()\n",
    "# save as YAML\n",
    "yaml_string = model.to_yaml()\n",
    "\n",
    "# model reconstruction from JSON:\n",
    "model = model_from_json(json_string)\n",
    "# model reconstruction from YAML\n",
    "model = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型的权重\n",
    "\n",
    "model.save_weights('my_model_weights.h5')\n",
    "\n",
    "#在代码中初始化一个完全相同的模型\n",
    "model.load_weights('my_model_weights.h5')\n",
    "#加载权重到不同的网络结构（有些层一样）\n",
    "model.load_weights('my_model_weights.h5', by_name=True)\n",
    "\n",
    "\"\"\"\n",
    "假如原模型为：\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=3, name=\"dense_1\"))\n",
    "    model.add(Dense(3, name=\"dense_2\"))\n",
    "    ...\n",
    "    model.save_weights(fname)\n",
    "\"\"\"\n",
    "# new model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=3, name=\"dense_1\"))  # will be loaded\n",
    "model.add(Dense(10, name=\"new_dense\"))  # will not be loaded\n",
    " \n",
    "# load weights from first model; will only affect the first layer, dense_1.\n",
    "model.load_weights(fname, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from numpy import argmax\n",
    "\n",
    "#数据为整型或整型字符的列表（数组）时，直接通过to_categorical编码\n",
    "int_labels=[0,2,1,3]\n",
    "# int_labels=['0','2','1','3']\n",
    "categorical_labels = to_categorical(int_labels, num_classes=4) #编码\n",
    "categorical_labels\n",
    "inverted = argmax(categorical_labels[0]) #反编码\n",
    "inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import argmax\n",
    "\n",
    "#数据为其他字符串类型的列表（数组）时，先通过LabelEncoder标准化标签，再通过to_categorical编码\n",
    "str_labels=['a','b','c','a']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(str_labels) #标准化标签，将标签值统一转换成range(标签值个数-1)范围内\n",
    "encoded_labels\n",
    "categorical_labels = to_categorical(encoded_labels, num_classes=3) #编码\n",
    "categorical_labels\n",
    "inverted = argmax(categorical_labels[0]) #反编码\n",
    "inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 案列一：神经网络解决回归问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1.Build the trainning data\n",
    "X=np.linspace(-1,1,200)\n",
    "np.random.shuffle(X)\n",
    "Y=0.5*X+2+np.random.normal(0,0.05,(200,))\n",
    "plt.scatter(X,Y)\n",
    "plt.show()\n",
    "X_train,Y_train=X[:160],Y[:160]\n",
    "X_test,Y_test=X[160:],Y[160:]\n",
    "print(X)\n",
    "print(\"*******************************************\")\n",
    "print(Y)\n",
    "\n",
    "# 2.Build a neural network from the 1st layer to the last layer\n",
    "model=Sequential()\n",
    "model.add(Dense(output_dim=1,input_dim=1))\n",
    "\n",
    "#3. Choose loss function and optimzing method\n",
    "model.compile(loss='mse',optimizer='sgd')\n",
    "\n",
    "#4. Trainning\n",
    "print(\"Training......\")\n",
    "for step in range(1400):\n",
    "        cost=model.train_on_batch(X_train,Y_train)\n",
    "        if step % 100 ==0:\n",
    "                print('train cost',cost)\n",
    "                \n",
    "#5.Test\n",
    "print(\"\\n Testing...........\")\n",
    "cost=model.evaluate(X_test,Y_test,batch_size=40)\n",
    "print(\"Test cost:\",cost)\n",
    "W,b=model.layers[0].get_weights()\n",
    "print('Weight=',W,\"\\nbiases=\",b)\n",
    "\n",
    "#6.Plotting the prediction\n",
    "Y_pred=model.predict(X_test)\n",
    "plt.scatter(X_test,Y_test)\n",
    "plt.plot(X_test,Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案列二：神经网络解决二类分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "#输入训练数据 keras接收numpy数组类型的数据\n",
    "x=np.array([[0,1,0],\n",
    "            [0,0,1],\n",
    "            [1,3,2],\n",
    "            [3,2,1]])\n",
    "y=np.array([0,0,1,1]).T\n",
    "#最简单的序贯模型，序贯模型是多个网络层的线性堆叠\n",
    "simple_model=Sequential()\n",
    "#dense层为全连接层\n",
    "#第一层隐含层为全连接层 5个神经元 输入数据的维度为3\n",
    "simple_model.add(Dense(5,input_dim=3,activation='relu'))\n",
    "#第二个隐含层 4个神经元\n",
    "simple_model.add(Dense(4,activation='relu'))\n",
    "#输出层为1个神经元\n",
    "simple_model.add(Dense(1,activation='sigmoid'))\n",
    "#编译模型,训练模型之前需要编译模型\n",
    "#编译模型的三个参数：优化器、损失函数、指标列表\n",
    "simple_model.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "#训练网络 2000次\n",
    "#Keras以Numpy数组作为输入数据和标签的数据类型。训练模型一般使用fit函数\n",
    "simple_model.fit(x,y,epochs=40,batch_size=128)\n",
    "#应用模型 进行预测\n",
    "y_=simple_model.predict_classes(x[0:1])\n",
    "print(\"[0,1,0]的分类结果：\"+str(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                1344      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,569\n",
      "Trainable params: 5,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.7362 - acc: 0.4990\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.7194 - acc: 0.4890\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.7146 - acc: 0.4900\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.7109 - acc: 0.4970\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.7054 - acc: 0.4930\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.7016 - acc: 0.5060\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.7004 - acc: 0.5020\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.6951 - acc: 0.5270\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6975 - acc: 0.5020\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.7017 - acc: 0.4800\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.6966 - acc: 0.5070\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.6995 - acc: 0.4970\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6983 - acc: 0.4980\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.6965 - acc: 0.4990\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 0.6996 - acc: 0.4970\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.6922 - acc: 0.5290\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.6973 - acc: 0.4910\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.6912 - acc: 0.5340\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 0.6932 - acc: 0.5300\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6931 - acc: 0.5070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xbd796a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 270us/step\n",
      "[0.68873894214630127, 0.60000002384185791]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation,Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#设置随机数生成器种子,实现结果的重现\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataframe = pandas.read_csv(\"data/sonar.csv\", header=None) # 加载数据集\n",
    "dataset = dataframe.values\n",
    "X = dataset[:, 0:60].astype(float) # 分割为60个输入变量\n",
    "Y = dataset[:, 60]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, encoded_Y, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30,input_dim=60, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(30, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(train_X, train_y, epochs=250, batch_size=7, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_X,test_y,verbose=1)\n",
    "print (\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案列三：神经网络解决多分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.4140 - acc: 0.1120\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 2.4050 - acc: 0.1050\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 2.3378 - acc: 0.1090\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 2.3336 - acc: 0.1050\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 2.3410 - acc: 0.1030\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 2.3241 - acc: 0.1090\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 2.3143 - acc: 0.1080\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 2.3101 - acc: 0.1180\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 2.3042 - acc: 0.1130\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 2.3101 - acc: 0.1090\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 2.3037 - acc: 0.1210\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 2.3010 - acc: 0.1170\n",
      "Epoch 13/40\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 2.3021 - acc: 0.1150\n",
      "Epoch 14/40\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 2.3029 - acc: 0.1180\n",
      "Epoch 15/40\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 2.2971 - acc: 0.1130\n",
      "Epoch 16/40\n",
      "1000/1000 [==============================] - 0s 87us/step - loss: 2.2948 - acc: 0.1160\n",
      "Epoch 17/40\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 2.3004 - acc: 0.1100\n",
      "Epoch 18/40\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 2.2956 - acc: 0.1180\n",
      "Epoch 19/40\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 2.2976 - acc: 0.1200\n",
      "Epoch 20/40\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 2.2962 - acc: 0.1230\n",
      "Epoch 21/40\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 2.2999 - acc: 0.1040\n",
      "Epoch 22/40\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 2.2948 - acc: 0.1260\n",
      "Epoch 23/40\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 2.2933 - acc: 0.1250\n",
      "Epoch 24/40\n",
      "1000/1000 [==============================] - 0s 83us/step - loss: 2.3013 - acc: 0.1140\n",
      "Epoch 25/40\n",
      "1000/1000 [==============================] - 0s 99us/step - loss: 2.2952 - acc: 0.1070\n",
      "Epoch 26/40\n",
      "1000/1000 [==============================] - 0s 112us/step - loss: 2.2855 - acc: 0.1440\n",
      "Epoch 27/40\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 2.2995 - acc: 0.1090\n",
      "Epoch 28/40\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 2.2896 - acc: 0.1230\n",
      "Epoch 29/40\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 2.2937 - acc: 0.1210\n",
      "Epoch 30/40\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 2.2930 - acc: 0.1270\n",
      "Epoch 31/40\n",
      "1000/1000 [==============================] - 0s 122us/step - loss: 2.2934 - acc: 0.1160\n",
      "Epoch 32/40\n",
      "1000/1000 [==============================] - 0s 117us/step - loss: 2.2921 - acc: 0.1160\n",
      "Epoch 33/40\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 2.2944 - acc: 0.1230\n",
      "Epoch 34/40\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 2.2840 - acc: 0.1320\n",
      "Epoch 35/40\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 2.2907 - acc: 0.1230\n",
      "Epoch 36/40\n",
      "1000/1000 [==============================] - 0s 114us/step - loss: 2.2886 - acc: 0.1310\n",
      "Epoch 37/40\n",
      "1000/1000 [==============================] - 0s 120us/step - loss: 2.2910 - acc: 0.1170 0s - loss: 2.2879 - acc: 0.121\n",
      "Epoch 38/40\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 2.2913 - acc: 0.1340\n",
      "Epoch 39/40\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 2.2936 - acc: 0.1240\n",
      "Epoch 40/40\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 2.2831 - acc: 0.1320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9bff41d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 14ms/step\n",
      "[2.2973096370697021, 0.12999999523162842]\n"
     ]
    }
   ],
   "source": [
    "##2.1引用包\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import keras\n",
    "##2.2生成数据\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "##2.3构建模型\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "##2.4训练模型\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=40,\n",
    "          batch_size=128)\n",
    "##2.5评估模型\n",
    "score=model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coding=utf-8\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def one_hot_encode_object_array(arr):\n",
    "    uniques, ids = np.unique(arr, return_inverse=True)\n",
    "    return np_utils.to_categorical(ids, len(uniques))\n",
    "\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "X = iris.values[:, :4]\n",
    "y = iris.values[:, 4]\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.6, random_state=0)\n",
    "train_y_ohe = one_hot_encode_object_array(train_y)\n",
    "test_y_ohe = one_hot_encode_object_array(test_y)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200,input_dim=4,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(train_X, train_y_ohe, epochs=20, batch_size=5, verbose=0)\n",
    "\n",
    "# predict_y = model.predict_proba(test_X)\n",
    "# print predict_y\n",
    "\n",
    "loss, accuracy = model.evaluate(test_X,test_y_ohe,verbose=1)\n",
    "print (\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KerasClassifier方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 1.0683 - acc: 0.3048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa409a470>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "135/135 [==============================] - 3s 25ms/step - loss: 0.9407 - acc: 0.6148\n",
      "15/15 [==============================] - 1s 88ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 3s 25ms/step - loss: 0.9734 - acc: 0.5407\n",
      "15/15 [==============================] - 1s 90ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 3s 25ms/step - loss: 1.1852 - acc: 0.3333\n",
      "15/15 [==============================] - 1s 90ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 3s 23ms/step - loss: 1.0193 - acc: 0.5259\n",
      "15/15 [==============================] - 1s 93ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 3s 23ms/step - loss: 1.1442 - acc: 0.3407\n",
      "15/15 [==============================] - 1s 92ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 3s 24ms/step - loss: 1.0124 - acc: 0.3778\n",
      "15/15 [==============================] - 1s 91ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 4s 26ms/step - loss: 1.0236 - acc: 0.4741\n",
      "15/15 [==============================] - 1s 90ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 3s 22ms/step - loss: 0.8964 - acc: 0.5704\n",
      "15/15 [==============================] - 2s 114ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 3s 25ms/step - loss: 0.9608 - acc: 0.6222\n",
      "15/15 [==============================] - 1s 91ms/step\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 4s 27ms/step - loss: 0.9419 - acc: 0.5926\n",
      "15/15 [==============================] - 2s 101ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.666667133569717"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.cnblogs.com/arkenstone/p/5943489.html?utm_source=itdadao&utm_medium=referral \n",
    "#利用Keras进行多类分类\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# load dataset\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "X = iris.values[:, :4]\n",
    "Y = iris.values[:, 4]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(Y) #标准化标签，将标签值统一转换成range(标签值个数-1)范围内\n",
    "# convert integers to dummy variables (one hot encoding)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# define model structure\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=200, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(units=3,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=20, batch_size=5)\n",
    "# splitting data into training set and test set. If random_state is set to an integer, the split datasets are fixed.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, dummy_y, test_size=0.3, random_state=0)\n",
    "estimator.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions\n",
    "pred = estimator.predict(X_test)\n",
    "\n",
    "# inverse numeric variables to initial categorical labels\n",
    "init_lables = encoder.inverse_transform(pred)\n",
    "\n",
    "# k-fold cross-validate\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "results.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
