{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV,cross_val_score,learning_curve,validation_curve\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr,randint\n",
    "from sklearn.datasets import load_iris,load_digits\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn官方文档：https://scikit-learn.org/stable/index.html  \n",
    "sklearn中文文档：http://sklearn.apachecn.org/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们拿到一批原始的数据首先要明确有多少特征，哪些是连续的，哪些是类别的。  \n",
    "检查有没有缺失值，对确实的特征选择恰当方式进行弥补，使数据完整。  \n",
    "对连续的数值型特征进行标准化，使得均值为0，方差为1。  \n",
    "对类别型的特征进行one-hot编码。  \n",
    "将需要转换成类别型数据的连续型数据进行二值化。  \n",
    "为防止过拟合或者其他原因，选择是否要将数据进行正则化。  \n",
    "在对数据进行初探之后发现效果不佳，可以尝试使用多项式方法，寻找非线性的关系。  \n",
    "根据实际问题分析是否需要对特征进行相应的函数转换。  \n",
    "准备好美美的数据将为我们寻找美美的模型如虎添翼  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于不同的原因，许多现实中的数据集都包含有缺失值，要么是空白的，要么使用NaNs或者其它的符号替代。这些数据无法直接使用scikit-learn分类器直接训练，所以需要进行处理。幸运地是，sklearn中的Imputer类提供了一些基本的方法来处理缺失值，如使用均值、中位值或者缺失值所在列中频繁出现的值来替换。也可使用pandas进行数据的这种处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.        ,  2.        ],\n",
       "       [ 6.        ,  3.66666667],\n",
       "       [ 7.        ,  6.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "# Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "imp.transform(X)\n",
    "\n",
    "# Imputer类同样支持稀疏矩阵\n",
    "# X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])\n",
    "# imp = Imputer(missing_values=0, strategy='mean', axis=0)\n",
    "# imp.fit(X)\n",
    "# X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])\n",
    "# imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 噪声处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式：(X-X_mean)/X_std 也叫z-score规范化（零均值规范化）,变换后各维特征有0均值，单位方差。  \n",
    "sklearn.preprocessing.scale(X, axis=0, with_mean=True,with_std=True,copy=True)  \n",
    "实际应用中，需要做特征标准化的常见情景：SVM'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 常规计算\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "(X-X_mean)/X_std\n",
    "\n",
    "# 使用reprocessing.scale()\n",
    "X_scale = preprocessing.scale(X)\n",
    "# X_scale.mean(axis=0) #标准化之后各列的均值为0\n",
    "# X_scale.std(axis=0) #标准化之后各列的方差为1\n",
    "\n",
    "# 一般会把train和test集放在一起做标准化，或者在train集上做标准化后，用同样的标准化器去标准化test集，此时可以用StandardScaler\n",
    "# scaler = preprocessing.StandardScaler().fit(train)\n",
    "# scaler.transform(train)\n",
    "# scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缩放至某一范围"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1、极大极小化：preprocessing.MinMaxScaler**    \n",
    "最小-最大规范化对原始数据进行线性变换，变换到[0,1]区间（也可以是其他固定最小最大值的区间）  \n",
    "使用这种方法的目的包括：1、对于方差非常小的属性可以增强其稳定性；2、维持稀疏矩阵中为0的条目。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.        ,  1.        ],\n",
       "       [ 1.        ,  0.5       ,  0.33333333],\n",
       "       [ 0.        ,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2、绝对值最大标准化：MaxAbsScaler **   \n",
    "数据会被规模化到[-1,1]之间。特征中所有数据都会除以最大值。  \n",
    "这个方法对那些已经中心化均值为0或者稀疏的数据有意义。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "max_abs_scaler .fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化的过程是将每个样本缩放到单位范数(每个样本的范数为1)，如果要使用如二次型(点积)或者其它核方法计算两个样本之间的相似性这个方法会很有用。该方法是文本分类和聚类分析中经常使用的向量空间模型（Vector Space Model)的基础。主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn.preprocessing.normalize()函数\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "preprocessing.normalize(X, norm='l2') #默认L2范数\n",
    "\n",
    "# 使用sklearn.preprocessing.Normalizer()类\n",
    "normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n",
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二值化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**preprocessing.Binarizer(threshold=0)：可以设置threshold阈值，结果数据值大于阈值的为1，小于阈值的为0**  \n",
    "特征二值化是将数值型的特征值转换为布尔值，可以用于概率估计。这在文本处理过程中也非常常见。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "binarizer = preprocessing.Binarizer().fit(X)\n",
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别特征one-hot编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当某些特征不是连续取值而是分类数据时，就需要对分类特征进行编码，比如人的性别有[“男”, “女”]之分，国籍可以是[“中国”, “英国”, “美国”]。这样的特征可以分别用不同的整数进行标记，比如[“男”, “女”]分表表示成[0, 1]，[“中国”, “英国”, “美国”]分别表示成[0, 1, 2]，但是，这种整数表示方法在模型中会被理解成不同的大小（比如2和0的距离比1和0大）。解决这一问题的一个方法是使用one-of-K或者one-hot编码，通过OneHotEncoder实现。这一估计量将每个含有m个取值的分类特征转化为m个二值特征，其中只有一个处于active状态。  \n",
    "两种哑编码的实现方法，pandas和sklearn。区别是pandas默认只处理字符串类别变量，sklearn默认只处理数值型类别变量(需要先 LabelEncoder )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pd.get_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.get_dummies(prefix=)  \n",
    "pandas的get_dummies()可以直接对变量进行one-hot编码，其中prefix是为one-hot编码后的变量进行命名。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoder不能直接对字符型变量编码，因此我们需要先将字符型变量通过LabelEncoder转换为数值型变量，如果本身为数值型则不用。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[ 0, 0, 3],\n",
    "     [ 1, 1, 0],\n",
    "     [ 0, 2, 1],\n",
    "     [ 1, 0, 2]]\n",
    "nc = preprocessing.OneHotEncoder().fit(X)\n",
    "nc.transform([[0,1,3]]).toarray()\n",
    "# array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])  # 一共9位\n",
    "# >>> # feature1只有0,1两个取值，因此是两位\n",
    "# >>> # feature2有0,1,2三个取值，因此是三位\n",
    "# >>> # feature3有0,1,2,3四个取值，因此是四位"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个特征的分类个数默认上根据输入数据集自动计算。但是也可以通过设置参数n_values进行人为设定。在上述例子中，三个分类属性的可选值数量分别为2，3，4。特别是当训练数据集在某个可能取值上没有训练样本时，需要人为制定分类数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values=[2, 3, 4], sparse=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = preprocessing.OneHotEncoder(n_values=[2, 3, 4])\n",
    "enc.fit([[1,2,3],[0,2,0]])\n",
    "enc.transform([[1 , 0, 0]]).toarray()\n",
    "#array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别特征LabelEncoder标签编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/jin_tmac/article/details/80611676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([1, 2, 2, 6])\n",
    "le.transform([1, 1, 2, 6]) #array([0, 0, 1, 2])\n",
    "# #非数值型转化为数值型\n",
    "le.fit([\"amsterdam\",\"paris\",\"paris\",\"tokyo\" ])\n",
    "le.transform([\"tokyo\",\"tokyo\",\"paris\"]) #array([2, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别特征人工处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['grade_id','enter_type','city_id','spr_code1','spr_code2','spr_code3']\n",
    "# 保存类别变量映射字典\n",
    "def save_categorical_dic(data):\n",
    "    categorical_dic = {}\n",
    "    for i in categorical_cols:\n",
    "        categorical_cnt = data[i].value_counts(normalize=True)\n",
    "        categorical_index = categorical_cnt[categorical_cnt>0.01].index.tolist()\n",
    "        categorical_index.append('others')\n",
    "        categorical_dic[i] = dict(zip(categorical_index,range(len(categorical_index))))\n",
    "    with open('categorical_dic.txt','w') as f:\n",
    "        f.write(str(categorical_dic))\n",
    "\n",
    "# 获取类别变量映射字典       \n",
    "def get_categorical_dic():\n",
    "    with open('categorical_dic.txt','r') as f:\n",
    "        a = f.read()\n",
    "        return eval(a)\n",
    "\n",
    "# 类别变量设置\n",
    "def set_categorical(data):\n",
    "    categorical_dic = get_categorical_dic()\n",
    "    for i in categorical_cols:\n",
    "        data[i] = data[i].map(lambda x:categorical_dic[i][x] if x in categorical_dic[i] else list(categorical_dic[i].values())[-1])\n",
    "    return data   \n",
    "\n",
    "# save_categorical_dic(data_train)\n",
    "data_train = set_categorical(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成多项式特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在输入数据存在非线性特征时，这一操作对增加模型的复杂度十分有用。一种常见的用法是生成多项式特征，能够得到特征的高阶项和相互作用项。多项式特征经常用于使用多项式核函数的核方法（比如SVC和KernelPCA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   0.,   0.,   1.],\n",
       "       [  1.,   2.,   3.,   4.,   6.,   9.],\n",
       "       [  1.,   4.,   5.,  16.,  20.,  25.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],\n",
       "       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],\n",
       "       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征向量X=(X1, X2)被转化为(1, X1, X2, X1^2, X1X2, X2^2)\n",
    "X=[[0, 1],\n",
    "   [2, 3],\n",
    "   [4, 5]]\n",
    "poly = preprocessing.PolynomialFeatures(2)\n",
    "poly.fit_transform(X)\n",
    "\n",
    "# 通过设定interaction_only=True仅实现相互作用项，X=(X1, X2, X3)被转化为to (1, X1, X2, X3, X1X2, X1X3, X2X3, X1X2X3)\n",
    "X=[[0, 1, 2],\n",
    "   [3, 4, 5],\n",
    "   [6, 7, 8]]\n",
    "poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据规约"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
